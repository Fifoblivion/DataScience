{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "\n",
    "# Features & Target\n",
    "X = df.iloc[:,:8].values\n",
    "y = df['class'].values\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "LR = LogisticRegression()\n",
    "RF = RandomForestClassifier(n_estimators = 100)\n",
    "SVM = SVC(random_state=0, probability=True)\n",
    "KNN = KNeighborsClassifier()\n",
    "DT = DecisionTreeClassifier()\n",
    "AdaBoost = AdaBoostClassifier(n_estimators = 100)\n",
    "Bagging = BaggingClassifier(n_estimators = 100)\n",
    "GBC = GradientBoostingClassifier(n_estimators = 100)\n",
    "\n",
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in zip([LR, RF, SVM, KNN, DT, AdaBoost, Bagging, GBC], \n",
    "                      ['Logistic Regression', 'Random Forest', 'Support Vector Machine','KNeighbors',\n",
    "                       'Decision Tree','Ada Boost','Bagging','Gradient Boosting']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above benchmarking we see that some models are giving better accuracy compared to other models. Let's combine non-similar models to create a robust generalized model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building only the best performance\n",
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in zip([DT, SVM, RF, Bagging, GBC], \n",
    "                      ['Decision Tree', 'Support Vector Machine', 'Random Forest',\n",
    "                       'Bagging','Gradient Boosting']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Voting vs Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VoteH = VotingClassifier(estimators=[('dt', DT),('svm', SVM), ('rf', RF), ('bag', Bagging), ('gnb', GBC)], voting='hard')\n",
    "VoteS = VotingClassifier(estimators=[('dt', DT),('svm', SVM), ('rf', RF), ('bag', Bagging), ('gnb', GBC)], voting='soft', weights=[1,1,1,1,1])\n",
    "\n",
    "for clf, label in zip([VoteH, VoteS], ['Ensemble Hard Voting', 'Ensemble Soft Voting']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
