{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Models\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Plot Styling\n",
    "import seaborn as sns; \n",
    "sns.set(); \n",
    "sns.set_style(\"whitegrid\")  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Delivery Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample dataset of delivery fleet driver data. \n",
    "\n",
    "For the sake of simplicity, we'll only be looking at two driver features: \n",
    "- mean distance driven per day\n",
    "- mean percentage of time a driver was >5 mph over the speed limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery = pd.read_csv('data/delivery.csv', sep='\\t')\n",
    "delivery.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(delivery.Distance_Feature,delivery.Speeding_Feature)\n",
    "plt.ylabel('Speeding Feature')\n",
    "plt.xlabel('Distance Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = delivery.drop('Driver_ID', axis=1)\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.scatter(X[y_kmeans == 0]['Distance_Feature'], X[y_kmeans == 0]['Speeding_Feature'], s = 50, c = 'red', label = 'Group 1')\n",
    "plt.scatter(X[y_kmeans == 1]['Distance_Feature'], X[y_kmeans == 1]['Speeding_Feature'], s = 50, c = 'blue', label = 'Group 2')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s = 100, c = 'yellow', label = 'Centroids')\n",
    "plt.xlabel('Distance Feature')\n",
    "plt.ylabel('Speeding Feature')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart below shows the results. Visually, you can see that the K-means algorithm splits the two groups based on the distance feature. Each cluster centroid is marked with a star.\n",
    "\n",
    "- Group 1 Centroid = (50, 8.8)\n",
    "- Group 2 Centroid = (180, 18.2)\n",
    "\n",
    "Using domain knowledge of the dataset, we can infer that **Group 1 is urban drivers** and **Group 2 is rural drivers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 4)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.scatter(X[y_kmeans == 0]['Distance_Feature'], X[y_kmeans == 0]['Speeding_Feature'], s = 50, c = 'red', label = 'Group 1')\n",
    "plt.scatter(X[y_kmeans == 1]['Distance_Feature'], X[y_kmeans == 1]['Speeding_Feature'], s = 50, c = 'blue', label = 'Group 2')\n",
    "plt.scatter(X[y_kmeans == 2]['Distance_Feature'], X[y_kmeans == 2]['Speeding_Feature'], s = 50, c = 'green', label = 'Group 3')\n",
    "plt.scatter(X[y_kmeans == 3]['Distance_Feature'], X[y_kmeans == 3]['Speeding_Feature'], s = 50, c = 'cyan', label = 'Group 4')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s = 100, c = 'yellow', label = 'Centroids')\n",
    "plt.xlabel('Distance Feature')\n",
    "plt.ylabel('Speeding Feature')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart below shows the resulting clusters. We see that four distinct groups have been identified by the algorithm, now speeding drivers have been separated from those who follow speed limits along with the rural vs. urban divide. \n",
    "\n",
    "The threshold for speeding is lower with the urban driver group than for the rural drivers, likely due to urban drivers spending more time in intersections and stop-and-go traffic.\n",
    "\n",
    "- Cluster 1 – Long distance, low speed\n",
    "- Cluster 2 – Short distance, low speed\n",
    "- Cluster 3 – Short distance, medium speed\n",
    "- Cluster 4 – Long distance, high speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2 Geyser's Eruptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Additional Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Library\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans on Geyser’s Eruptions Segmentation\n",
    "We’ll first implement the kmeans algorithm on 2D dataset and see how it works. The dataset has 272 observations and 2 features. The data covers the waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. We will try to find K subgroups within the data points and group them accordingly. Below is the description of the features:\n",
    "\n",
    "- eruptions (float): Eruption time in minutes.\n",
    "- waiting (int): Waiting time to next eruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geyser = pd.read_csv('data/old_faithful_geyser.csv')\n",
    "geyser.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(geyser['Eruption_length_(mins)'], geyser['Eruption_wait_(mins)'])\n",
    "plt.xlabel('Eruption time in mins')\n",
    "plt.ylabel('Waiting time to next eruption')\n",
    "plt.title('Visualization of raw data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "X = StandardScaler().fit_transform(geyser.drop('Index',axis=1))\n",
    "\n",
    "# Modeling\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Group 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Group 2')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s = 100, c = 'yellow', label = 'Centroids')\n",
    "plt.xlabel('Eruption time in mins')\n",
    "plt.ylabel('Waiting time to next eruption')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Visualization of clustered data', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Kmeans algorithm and get the index of data points clusters\n",
    "sse = []\n",
    "list_k = list(range(1, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k).fit(X)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "X = StandardScaler().fit_transform(geyser.drop('Index',axis=1))\n",
    "\n",
    "# Modeling\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Group 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Group 2')\n",
    "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Group 1')\n",
    "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 50, c = 'cyan', label = 'Group 2')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s = 100, c = 'yellow', label = 'Centroids')\n",
    "plt.xlabel('Eruption time in mins')\n",
    "plt.ylabel('Waiting time to next eruption')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Visualization of clustered data', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.datasets.samples_generator import (make_blobs,make_circles,make_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = make_blobs(n_samples=300, centers=4,cluster_std=0.60, random_state=0)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Kmeans algorithm and get the index of data points clusters\n",
    "\n",
    "sse = []\n",
    "list_k = list(range(1, 15))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k).fit(X)\n",
    "    sse.append(km.inertia_)\n",
    "    \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above the elbow is at k=4 indicating the optimal k for this dataset is 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4).fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering K = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(6, random_state=0).fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis');\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawback\n",
    "Kmeans algorithm is good in capturing structure of the data if clusters have a spherical-like shape. It always try to construct a nice spherical shape around the centroid. That means, the minute the clusters have a complicated geometric shapes, kmeans does a poor job in clustering the data. We’ll illustrate three cases where kmeans will not perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cricles\n",
    "X1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)\n",
    "\n",
    "# Moons\n",
    "X2 = make_moons(n_samples=1500, noise=0.05)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "for i, X in enumerate([X1, X2]):\n",
    "    fig.set_size_inches(18, 7)\n",
    "    km = KMeans(n_clusters=2).fit(X[0])\n",
    "    labels = km.predict(X[0])\n",
    "    centroids = km.cluster_centers_\n",
    "\n",
    "    ax[i].scatter(X[0][:, 0], X[0][:, 1], c=[colors[l_] for l_ in labels])\n",
    "    ax[i].scatter(centroids[0, 0], centroids[0, 1], marker='*', s=400, c='y')\n",
    "    ax[i].scatter(centroids[1, 0], centroids[1, 1], marker='+', s=300, c='g')\n",
    "plt.suptitle('Simulated data', y=1.05, fontsize=22, fontweight='semibold')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One version of this kernelized k-means is implemented in Scikit-Learn within the SpectralClustering estimator. It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a k-means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cricles\n",
    "X1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)\n",
    "\n",
    "# Moons\n",
    "X2 = make_moons(n_samples=1500, noise=0.05)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "for i, X in enumerate([X1, X2]):\n",
    "    fig.set_size_inches(18, 7)\n",
    "    sp = SpectralClustering(n_clusters=2, affinity='nearest_neighbors').fit(X[0])\n",
    "    labels = sp.labels_\n",
    "    ax[i].scatter(X[0][:, 0], X[0][:, 1], c=[colors[l_] for l_ in labels])\n",
    "plt.suptitle('Simulated data', y=1.05, fontsize=22, fontweight='semibold')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
