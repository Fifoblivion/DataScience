{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "\n",
    "# Features & Target\n",
    "X = df.iloc[:,:8].values\n",
    "y = df['class'].values\n",
    "\n",
    "# Normalize\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Decision Trees for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# K-fold 5 splits\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "# Decision Tree with 5 fold cross validation\n",
    "DT = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "results = cross_val_score(DT, X_train,y_train, cv=kfold)\n",
    "print (\"Decision Tree (stand alone) - Train : \", results.mean())\n",
    "print (\"Decision Tree (stand alone) - Test : \", metrics.accuracy_score(DT.predict(X_test), y_test))\n",
    "\n",
    "# Using Bagging and build 100 decision tree models\n",
    "bag_DT = BaggingClassifier(base_estimator=DT, n_estimators=num_trees).fit(X_train,y_train)\n",
    "results = cross_val_score(bag_DT, X_train, y_train, cv=kfold)\n",
    "print (\"\\nDecision Tree (Bagging) - Train : \", results.mean())\n",
    "print (\"Decision Tree (Bagging) - Test : \", metrics.accuracy_score(bag_DT.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = DT.feature_importances_\n",
    "\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, df.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# K-fold 5 splits\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=num_trees).fit(X_train, y_train)\n",
    "results = cross_val_score(RF, X_train, y_train, cv=kfold)\n",
    "\n",
    "print (\"\\nRandom Forest - Train : \", results.mean())\n",
    "print (\"Random Forest - Test : \", metrics.accuracy_score(RF.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# K-fold 5 splits\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "ET = ExtraTreesClassifier(n_estimators=num_trees).fit(X_train, y_train)\n",
    "results = cross_val_score(ET, X_train, y_train, cv=kfold)\n",
    "\n",
    "print (\"\\nExtraTree - Train : \", results.mean())\n",
    "print (\"ExtraTree - Test : \", metrics.accuracy_score(ET.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary Looks Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new X with only 2 columns\n",
    "X = X[:, -2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2017)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "# Decision Tree with 5 fold cross validation\n",
    "DT = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "results = cross_val_score(DT, X_train,y_train, cv=kfold)\n",
    "print (\"Decision Tree (stand alone) - Train : \", results.mean())\n",
    "print (\"Decision Tree (stand alone) - Test : \", metrics.accuracy_score(DT.predict(X_test), y_test))\n",
    "\n",
    "# Using Bagging and build 100 decision tree models\n",
    "bag_DT = BaggingClassifier(base_estimator=DT, n_estimators=num_trees).fit(X_train,y_train)\n",
    "results = cross_val_score(bag_DT, X_train, y_train, cv=kfold)\n",
    "print (\"\\nDecision Tree (Bagging) - Train : \", results.mean())\n",
    "print (\"Decision Tree (Bagging) - Test : \", metrics.accuracy_score(bag_DT.predict(X_test), y_test))\n",
    "\n",
    "# Using Random Forest with 100 trees\n",
    "RF = RandomForestClassifier(n_estimators=num_trees).fit(X_train, y_train)\n",
    "results = cross_val_score(RF, X_train, y_train, cv=kfold)\n",
    "\n",
    "print (\"\\nRandom Forest - Train : \", results.mean())\n",
    "print (\"Random Forest  - Test : \", metrics.accuracy_score(RF.predict(X_test), y_test))\n",
    "\n",
    "# Using Extra Trees with 100 trees\n",
    "ET = ExtraTreesClassifier(n_estimators=num_trees).fit(X_train, y_train)\n",
    "results = cross_val_score(ET, X_train, y_train, cv=kfold)\n",
    "\n",
    "print (\"\\nExtraTree - Train : \", results.mean())\n",
    "print (\"ExtraTree - Test : \", metrics.accuracy_score(ET.predict(X_test), y_test))\n",
    "\n",
    "def plot_decision_regions(X, y, classifier):\n",
    "    h = .02  \n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h),np.arange(x2_min, x2_max, h))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8,marker=markers[idx], label=cl)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(221)\n",
    "plot_decision_regions(X, y, DT)\n",
    "plt.title('Decision Tree (Stand alone)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "plt.subplot(222)\n",
    "plot_decision_regions(X, y, bag_DT)\n",
    "plt.title('Decision Tree (Bagging - 100 trees)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(223)\n",
    "plot_decision_regions(X, y, RF)\n",
    "plt.title('RandomForest Tree (100 trees)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(224)\n",
    "plot_decision_regions(X, y, ET)\n",
    "plt.title('Extreme Random Tree (100 trees)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
