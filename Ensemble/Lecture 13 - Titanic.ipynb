{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('data/titanic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Embarked\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Value Sex\n",
    "titanic[\"Sex\"] = titanic[\"Sex\"].map({\"male\": 0, \"female\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Age\n",
    "index_NaN_age = list(titanic[\"Age\"][titanic[\"Age\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age :\n",
    "    age_med = titanic[\"Age\"].median()\n",
    "    age_pred = titanic[\"Age\"][((titanic['SibSp'] == titanic.iloc[i][\"SibSp\"]) & (titanic['Parch'] == titanic.iloc[i][\"Parch\"]) & (titanic['Pclass'] == titanic.iloc[i][\"Pclass\"]))].median()\n",
    "    if not np.isnan(age_pred) :\n",
    "        titanic['Age'].iloc[i] = age_pred\n",
    "    else :\n",
    "        titanic['Age'].iloc[i] = age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Title\n",
    "titanic_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in titanic[\"Name\"]]\n",
    "titanic[\"Title\"] = pd.Series(titanic_title)\n",
    "titanic[\"Title\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Title to categorical values \n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "titanic[\"Title\"] = titanic[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
    "titanic[\"Title\"] = titanic[\"Title\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Name\n",
    "titanic.drop(labels = [\"Name\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Family Size\n",
    "titanic[\"Fsize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Single, Small Family, Medium Family, Large Family Columns\n",
    "titanic['Single'] = titanic['Fsize'].map(lambda s: 1 if s == 1 else 0)\n",
    "titanic['SmallF'] = titanic['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n",
    "titanic['MedF'] = titanic['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "titanic['LargeF'] = titanic['Fsize'].map(lambda s: 1 if s >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder Title & Embarked\n",
    "titanic = pd.get_dummies(titanic, columns = [\"Title\"])\n",
    "titanic = pd.get_dummies(titanic, columns = [\"Embarked\"], prefix=\"Em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Cabin\n",
    "titanic[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in titanic['Cabin'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder Cabin\n",
    "titanic = pd.get_dummies(titanic, columns = [\"Cabin\"],prefix=\"Cabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill and Convert Ticket\n",
    "Ticket = []\n",
    "for i in list(titanic.Ticket):\n",
    "    if not i.isdigit() :\n",
    "        Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) #Take prefix\n",
    "    else:\n",
    "        Ticket.append(\"X\")\n",
    "        \n",
    "titanic[\"Ticket\"] = Ticket\n",
    "titanic[\"Ticket\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder Ticket\n",
    "titanic = pd.get_dummies(titanic, columns = [\"Ticket\"], prefix=\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder Pclass\n",
    "titanic[\"Pclass\"] = titanic[\"Pclass\"].astype(\"category\")\n",
    "titanic = pd.get_dummies(titanic, columns = [\"Pclass\"],prefix=\"Pc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop PassengerId\n",
    "titanic.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic.drop(labels = [\"Survived\"],axis = 1)\n",
    "y = titanic[\"Survived\"]\n",
    "\n",
    "# Normalize\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Single vs Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = 100\n",
    "\n",
    "#Logreg\n",
    "LR = LogisticRegression(solver='lbfgs',max_iter = 4000).fit(X_train,y_train)\n",
    "results = cross_val_score(LR, X_train,y_train, cv=kfold)\n",
    "print (\"Logistic Regression (stand alone) - Train : \", results.mean())\n",
    "print (\"Logistic Regression (stand alone) - Test : \", metrics.accuracy_score(LR.predict(X_test), y_test))\n",
    "\n",
    "bag_LR = BaggingClassifier(base_estimator=LR, n_estimators=num_trees).fit(X_train,y_train)\n",
    "results = cross_val_score(bag_LR, X_train, y_train, cv=kfold)\n",
    "print (\"\\nLogistic Regression (Bagging) - Train : \", results.mean())\n",
    "print (\"Logistic Regression (Bagging) - Test : \", metrics.accuracy_score(bag_LR.predict(X_test), y_test))\n",
    "\n",
    "#KNN\n",
    "KNN = KNeighborsClassifier().fit(X_train,y_train)\n",
    "results = cross_val_score(KNN, X_train,y_train, cv=kfold)\n",
    "print (\"\\nKNN (stand alone) - Train : \", results.mean())\n",
    "print (\"KNN (stand alone) - Test : \", metrics.accuracy_score(KNN.predict(X_test), y_test))\n",
    "\n",
    "bag_KNN = BaggingClassifier(base_estimator=KNN, n_estimators=num_trees).fit(X_train,y_train)\n",
    "results = cross_val_score(bag_KNN, X_train, y_train, cv=kfold)\n",
    "print (\"\\nKNN (Bagging) - Train : \", results.mean())\n",
    "print (\"KNN (Bagging) - Test : \", metrics.accuracy_score(bag_KNN.predict(X_test), y_test))\n",
    "\n",
    "#SVM\n",
    "SVM = SVC(gamma='scale').fit(X_train,y_train)\n",
    "results = cross_val_score(SVM, X_train,y_train, cv=kfold)\n",
    "print (\"\\nSVM (stand alone) - Train : \", results.mean())\n",
    "print (\"SVM (stand alone) - Test : \", metrics.accuracy_score(SVM.predict(X_test), y_test))\n",
    "\n",
    "bag_SVM = BaggingClassifier(base_estimator=SVM, n_estimators=num_trees).fit(X_train,y_train)\n",
    "results = cross_val_score(bag_SVM, X_train, y_train, cv=kfold)\n",
    "print (\"\\nSVM (Bagging) - Train : \", results.mean())\n",
    "print (\"SVM (Bagging) - Test : \", metrics.accuracy_score(bag_SVM.predict(X_test), y_test))\n",
    "\n",
    "#Decision Tree\n",
    "DT = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "results = cross_val_score(DT, X_train,y_train, cv=kfold)\n",
    "print (\"\\nDecision Tree (stand alone) - Train : \", results.mean())\n",
    "print (\"Decision Tree (stand alone) - Test : \", metrics.accuracy_score(DT.predict(X_test), y_test))\n",
    "\n",
    "bag_DT = BaggingClassifier(base_estimator=DT, n_estimators=num_trees).fit(X_train,y_train)\n",
    "results = cross_val_score(bag_DT, X_train, y_train, cv=kfold)\n",
    "print (\"\\nDecision Tree (Bagging) - Train : \", results.mean())\n",
    "print (\"Decision Tree (Bagging) - Test : \", metrics.accuracy_score(bag_DT.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree - AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier()\n",
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[1,2],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsadaDTC.fit(X_train,y_train)\n",
    "\n",
    "ada_best = gsadaDTC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsadaDTC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier()\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "rf_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsRFC.fit(X_train,y_train)\n",
    "\n",
    "RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsRFC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExtraTrees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtC = ExtraTreesClassifier()\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "ex_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsExtC.fit(X_train,y_train)\n",
    "\n",
    "ExtC_best = gsExtC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC = GradientBoostingClassifier()\n",
    "gb_param_grid = {'loss' : [\"deviance\"],\n",
    "              'n_estimators' : [100,200,300],\n",
    "              'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'max_depth': [4, 8],\n",
    "              'min_samples_leaf': [100,150],\n",
    "              'max_features': [0.3, 0.1] \n",
    "              }\n",
    "\n",
    "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsGBC.fit(X_train,y_train)\n",
    "\n",
    "GBC_best = gsGBC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVC classifier\n",
    "SVMC = SVC(probability=True)\n",
    "svc_param_grid = {'kernel': ['rbf'], \n",
    "                  'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "                  'C': [1, 10, 50, 100,200,300, 1000]}\n",
    "\n",
    "gsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsSVMC.fit(X_train,y_train)\n",
    "\n",
    "SVMC_best = gsSVMC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsSVMC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "g = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsExtC.best_estimator_,\"Extra Trees learning curves\",X_train,y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,y_train,cv=kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Important of Tree Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n",
    "\n",
    "names_classifiers = [(\"AdaBoosting\", ada_best),(\"RandomForest\",RFC_best),(\"ExtraTrees\",ExtC_best),(\"GradientBoosting\",GBC_best)]\n",
    "\n",
    "nclassifier = 0\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        name = names_classifiers[nclassifier][0]\n",
    "        classifier = names_classifiers[nclassifier][1]\n",
    "        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n",
    "        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n",
    "        g.set_xlabel(\"Relative importance\",fontsize=12)\n",
    "        g.set_ylabel(\"Features\",fontsize=12)\n",
    "        g.tick_params(labelsize=9)\n",
    "        g.set_title(name + \" feature importance\")\n",
    "        nclassifier += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "VoteH = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best), ('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='hard', n_jobs=4)\n",
    "VoteS = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best), ('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n",
    "\n",
    "for clf, label in zip([VoteH, VoteS], ['Ensemble Hard Voting', 'Ensemble Soft Voting']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = 100\n",
    "verbose = True # to print the progress\n",
    "\n",
    "clfs = [RFC_best,ExtC_best,SVMC_best,ada_best,GBC_best]\n",
    "\n",
    "# Creating train and test sets for blending\n",
    "dataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_test.shape[0], len(clfs)))\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "for i, clf in enumerate(clfs):   \n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    print(\"##### Base Model %0.0f #####\" % i)\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "    clf.fit(X_train, y_train)   \n",
    "    print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_train), y_train)))\n",
    "    dataset_blend_train[:,i] = clf.predict_proba(X_train)[:, 1]\n",
    "    dataset_blend_test[:,i] = clf.predict_proba(X_test)[:, 1]\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))    \n",
    "\n",
    "print (\"##### Meta Model #####\")\n",
    "clf = LogisticRegression()\n",
    "scores = cross_val_score(clf, dataset_blend_train, y_train, cv=kfold, scoring='accuracy')\n",
    "clf.fit(dataset_blend_train, y_train)\n",
    "print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_train), y_train)))\n",
    "print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
